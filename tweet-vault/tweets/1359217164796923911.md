---
id: 1359217164796923911
author: yudapearl
published_date: 2021-02-09 19:06:39 +00:00
conversation_id: 1359217164796923911
in_reply_to: None
retweet: None
quoted_tweet: 1359193690821062658
type: tweet
tags:
keywords:
- why
- begging
- strategy
- machine
- folks
- learning
- sure
- program
- explain
- ahead
- even
- model
- i
- internalized
- reality
- me
- today
- matters

---

Machine learning folks were way ahead of me begging for "explainable ML". But I am not sure even today they have internalized that there is no such a thing as "explainable" program w/o a model of reality. You can explain the program's data-fitting strategy but not why it matters. https://t.co/x8y71qbLfD

[View tweet on Twitter.com](https://twitter.com/yudapearl/status/1359217164796923911)

### Metadata

Author: [[@yudapearl]]
Conversation: [[conversation-1359217164796923911]]
In reply to: [[None]]
Retweet of: [[None]]
Quoted tweet: [[1359193690821062658]]
Published Date: [[calendar/2021-02-09]]
keywords:
- [[index/why|why]]
- [[index/begging|begging]]
- [[index/strategy|strategy]]
- [[index/machine|machine]]
- [[index/folks|folks]]
- [[index/learning|learning]]
- [[index/sure|sure]]
- [[index/program|program]]
- [[index/explain|explain]]
- [[index/ahead|ahead]]
- [[index/even|even]]
- [[index/model|model]]
- [[index/i|i]]
- [[index/internalized|internalized]]
- [[index/reality|reality]]
- [[index/me|me]]
- [[index/today|today]]
- [[index/matters|matters]]
