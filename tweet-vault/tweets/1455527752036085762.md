---
id: 1455527752036085762
author: yudapearl
published_date: 2021-11-02 13:30:33 +00:00
conversation_id: 1455527752036085762
in_reply_to: None
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- understand
- could
- dags
- language
- would
- paper
- confusion
- explain
- hoped
- i
- anyone
- using
- articulated
- crossed
- unfortunately

---

This paper has crossed my desk: https://t.co/IO9VFNDiAn
which I've hoped would explain what the "causal confusion" is in "imitation learning," so it can be de-confused using DAGs. Unfortunately, the confusion is not articulated in a language I could understand. Anyone can help?

[View tweet on Twitter.com](https://twitter.com/yudapearl/status/1455527752036085762)

### Metadata

Author: [[@yudapearl]]
Conversation: [[conversation-1455527752036085762]]
In reply to: [[None]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2021-11-02]]
keywords:
- [[index/understand|understand]]
- [[index/could|could]]
- [[index/dags|dags]]
- [[index/language|language]]
- [[index/would|would]]
- [[index/paper|paper]]
- [[index/confusion|confusion]]
- [[index/explain|explain]]
- [[index/hoped|hoped]]
- [[index/i|i]]
- [[index/anyone|anyone]]
- [[index/using|using]]
- [[index/articulated|articulated]]
- [[index/crossed|crossed]]
- [[index/unfortunately|unfortunately]]
