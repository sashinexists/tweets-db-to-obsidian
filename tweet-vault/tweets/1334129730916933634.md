---
id: 1334129730916933634
author: hildeweerts
published_date: 2020-12-02 13:38:09 +00:00
conversation_id: 1334118825827168257
in_reply_to: 1334127849490370565
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- explanation
- explanations
- problem
- real
- ml
- interventions
- seems
- misinterpret
- correct
- main
- model
- people
- i
- types
- something
- used

---

@yudapearl It seems that your main problem with these types of explanations is that people might *misinterpret* an explanation from a first-rung ML model as something that can be used for interventions in the real (causal!) world? Is that correct or did I misinterpret your point?

[View tweet on Twitter.com](https://twitter.com/hildeweerts/status/1334129730916933634)

### Metadata

Author: [[@hildeweerts]]
Conversation: [[conversation-1334118825827168257]]
In reply to: [[1334127849490370565]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2020-12-02]]
keywords:
- [[index/explanation|explanation]]
- [[index/explanations|explanations]]
- [[index/problem|problem]]
- [[index/real|real]]
- [[index/ml|ml]]
- [[index/interventions|interventions]]
- [[index/seems|seems]]
- [[index/misinterpret|misinterpret]]
- [[index/correct|correct]]
- [[index/main|main]]
- [[index/model|model]]
- [[index/people|people]]
- [[index/i|i]]
- [[index/types|types]]
- [[index/something|something]]
- [[index/used|used]]
