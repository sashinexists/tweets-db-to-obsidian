---
id: 1334129730916933634
author: 1083328356911931392
published_date: 2020-12-02 13:38:09 +00:00
conversation_id: 1334118825827168257
---
@yudapearl It seems that your main problem with these types of explanations is that people might *misinterpret* an explanation from a first-rung ML model as something that can be used for interventions in the real (causal!) world? Is that correct or did I misinterpret your point?