---
id: 1334129730916933634
author: 1083328356911931392
published_date: 2020-12-02 13:38:09 +00:00
conversation_id: [[Conversation-1334118825827168257]]
in_reply_to: [[1334127849490370565]]
retweet: None
quoted_tweet: None
---
@yudapearl It seems that your main problem with these types of explanations is that people might *misinterpret* an explanation from a first-rung ML model as something that can be used for interventions in the real (causal!) world? Is that correct or did I misinterpret your point?
