---
id: 1339285683173732356
author: FelixHill84
published_date: 2020-12-16 19:06:04 +00:00
conversation_id: 1339285683173732356
in_reply_to: None
retweet: None
quoted_tweet: 1339243288705032192
type: tweet
tags:
keywords:
- equals
- learning
- cater
- models
- model
- simple
- built
- videos
- soft
- less
- datasets
- exploiting
- data
- estimation

---

We built a simple model for spatio-temporal QA from videos on CLEVRER and and CATER datasets, exploiting a soft 'object' estimation (MONet), self-attention and self-supervised (BERT-style) learning.

It equals the (SOTA) perf of neuro-symbolic models with 40% less labelled data https://t.co/u2AvqWp1yk

[View tweet on Twitter.com](https://twitter.com/FelixHill84/status/1339285683173732356)

### Metadata

Author: [[@FelixHill84]]
Conversation: [[conversation-1339285683173732356]]
In reply to: [[None]]
Retweet of: [[None]]
Quoted tweet: [[1339243288705032192]]
Published Date: [[calendar/2020-12-16]]
keywords:
- [[index/equals|equals]]
- [[index/learning|learning]]
- [[index/cater|cater]]
- [[index/models|models]]
- [[index/model|model]]
- [[index/simple|simple]]
- [[index/built|built]]
- [[index/videos|videos]]
- [[index/soft|soft]]
- [[index/less|less]]
- [[index/datasets|datasets]]
- [[index/exploiting|exploiting]]
- [[index/data|data]]
- [[index/estimation|estimation]]
