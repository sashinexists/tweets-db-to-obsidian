---
id: 1297721579325894657
author: GaryMarcus
published_date: 2020-08-24 02:25:09 +00:00
conversation_id: 1296678240271171585
in_reply_to: 1296678240271171585
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- between
- fooled
- become
- fluent
- great
- immense
- human
- meaningless
- ml
- compare
- example
- gets
- than
- contrast
- biology
- child
- things
- learns
- stark
- relations
- easily

---

@neuro_data @KordingLab @tdverstynen @danilobzdok @lakens @TonyZador @ylecun @criticalneuro @IrisVanRooij @neurograce @achristensen56 @AudeOliva @BlackInNeuro @BlackWomenInAI @tyrell_turing Biology learns about things in the world; ML learns about relations between meaningless tokens. The contrast is stark when you compare GPT with a human child. GPT can (with immense corpus) become more fluent than a child, but it easily gets fooled. Here's a great example [1/2]

[View tweet on Twitter.com](https://twitter.com/GaryMarcus/status/1297721579325894657)

### Metadata

Author: [[@GaryMarcus]]
Conversation: [[conversation-1296678240271171585]]
In reply to: [[1296678240271171585]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2020-08-24]]
keywords:
- [[index/between|between]]
- [[index/fooled|fooled]]
- [[index/become|become]]
- [[index/fluent|fluent]]
- [[index/great|great]]
- [[index/immense|immense]]
- [[index/human|human]]
- [[index/meaningless|meaningless]]
- [[index/ml|ml]]
- [[index/compare|compare]]
- [[index/example|example]]
- [[index/gets|gets]]
- [[index/than|than]]
- [[index/contrast|contrast]]
- [[index/biology|biology]]
- [[index/child|child]]
- [[index/things|things]]
- [[index/learns|learns]]
- [[index/stark|stark]]
- [[index/relations|relations]]
- [[index/easily|easily]]
