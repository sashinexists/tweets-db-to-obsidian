---
id: 1361486445488332804
author: rdisipio
published_date: 2021-02-16 01:23:58 +00:00
conversation_id: 1361486445488332804
in_reply_to: None
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- mechanism
- causal
- familiar
- machine
- deep
- equivalent
- neural
- attention
- concept
- networks

---

@yudapearl are you familiar with the concept of multi-head attention mechanism in Transformer deep neural networks and machine translation? Is there any equivalent in causal inference?

[View tweet on Twitter.com](https://twitter.com/rdisipio/status/1361486445488332804)

### Metadata

Author: [[@rdisipio]]
Conversation: [[conversation-1361486445488332804]]
In reply to: [[None]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2021-02-16]]
keywords:
- [[index/mechanism|mechanism]]
- [[index/causal|causal]]
- [[index/familiar|familiar]]
- [[index/machine|machine]]
- [[index/deep|deep]]
- [[index/equivalent|equivalent]]
- [[index/neural|neural]]
- [[index/attention|attention]]
- [[index/concept|concept]]
- [[index/networks|networks]]
