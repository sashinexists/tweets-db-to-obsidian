---
id: 1071545106757574656
author: tyrell_turing
published_date: 2018-12-08 23:20:36 +00:00
conversation_id: 1071210391101935616
in_reply_to: 1071540140567343104
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- product
- nns
- end
- study
- perspective
- problem
- ml
- learning
- because
- algorithm
- opinion
- yes
- often
- informative
- matters
- representations

---

@neurowitz @pfau Yes, in my opinion there is. The problem is the one @KordingLab highlights: neuroscientists study representations, but representations are often not informative from a NNs or ML perspective, because it's the learning algorithm that matters, not the end product of that algorithm.

[View tweet on Twitter.com](https://twitter.com/tyrell_turing/status/1071545106757574656)

### Metadata

Author: [[@tyrell_turing]]
Conversation: [[conversation-1071210391101935616]]
In reply to: [[1071540140567343104]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2018-12-08]]
keywords:
- [[index/product|product]]
- [[index/nns|nns]]
- [[index/end|end]]
- [[index/study|study]]
- [[index/perspective|perspective]]
- [[index/problem|problem]]
- [[index/ml|ml]]
- [[index/learning|learning]]
- [[index/because|because]]
- [[index/algorithm|algorithm]]
- [[index/opinion|opinion]]
- [[index/yes|yes]]
- [[index/often|often]]
- [[index/informative|informative]]
- [[index/matters|matters]]
- [[index/representations|representations]]
