---
id: 1151633995941240832
author: djvanness
published_date: 2019-07-17 23:25:15 +00:00
conversation_id: 1150906343416586241
in_reply_to: 1151633315344060417
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- does
- why
- concerned
- causal
- problem
- question
- exactly
- logical
- model
- simple
- i
- meaningful
- dag
- toy
- represent

---

@ConiByera @omaclaren @AndersHuitfeldt @yudapearl Exactly - which is why I am concerned that the DAG does not represent a meaningful causal model for this toy problem. The question remains: is there a DAG that can represent this simple logical interaction?

[View tweet on Twitter.com](https://twitter.com/djvanness/status/1151633995941240832)

### Metadata

Author: [[@djvanness]]
Conversation: [[conversation-1150906343416586241]]
In reply to: [[1151633315344060417]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2019-07-17]]
keywords:
- [[index/does|does]]
- [[index/why|why]]
- [[index/concerned|concerned]]
- [[index/causal|causal]]
- [[index/problem|problem]]
- [[index/question|question]]
- [[index/exactly|exactly]]
- [[index/logical|logical]]
- [[index/model|model]]
- [[index/simple|simple]]
- [[index/i|i]]
- [[index/meaningful|meaningful]]
- [[index/dag|dag]]
- [[index/toy|toy]]
- [[index/represent|represent]]
