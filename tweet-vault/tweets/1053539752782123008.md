---
id: 1053539752782123008
author: RandomlyWalking
published_date: 2018-10-20 06:53:45 +00:00
conversation_id: 1052680788389507073
in_reply_to: 1053533521921269761
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- does
- know
- agree
- all
- causal
- engineers
- problem
- implicit
- important
- model
- lots
- big
- either
- people
- i
- want
- hard
- aim
- computer
- define
- software

---

@yudapearl @tdietterich Lots of people who do "interpretable ML" agree that a big problem is that it's hard to know what we want. OTOH I can't define "clear computer program" either, but that does seem an important thing to aim at. Do all software engineers have implicit causal model of their code?

[View tweet on Twitter.com](https://twitter.com/RandomlyWalking/status/1053539752782123008)

### Metadata

Author: [[@RandomlyWalking]]
Conversation: [[conversation-1052680788389507073]]
In reply to: [[1053533521921269761]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2018-10-20]]
keywords:
- [[index/does|does]]
- [[index/know|know]]
- [[index/agree|agree]]
- [[index/all|all]]
- [[index/causal|causal]]
- [[index/engineers|engineers]]
- [[index/problem|problem]]
- [[index/implicit|implicit]]
- [[index/important|important]]
- [[index/model|model]]
- [[index/lots|lots]]
- [[index/big|big]]
- [[index/either|either]]
- [[index/people|people]]
- [[index/i|i]]
- [[index/want|want]]
- [[index/hard|hard]]
- [[index/aim|aim]]
- [[index/computer|computer]]
- [[index/define|define]]
- [[index/software|software]]
