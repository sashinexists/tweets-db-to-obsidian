---
id: 1053539752782123008
author: 21815759
published_date: 2018-10-20 06:53:45 +00:00
conversation_id: [[Conversation-1052680788389507073]]
in_reply_to: [[1053533521921269761]]
retweet: None
quoted_tweet: None
---
@yudapearl @tdietterich Lots of people who do "interpretable ML" agree that a big problem is that it's hard to know what we want. OTOH I can't define "clear computer program" either, but that does seem an important thing to aim at. Do all software engineers have implicit causal model of their code?
