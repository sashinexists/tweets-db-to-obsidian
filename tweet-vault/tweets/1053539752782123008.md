---
id: 1053539752782123008
author: 21815759
published_date: 2018-10-20 06:53:45 +00:00
conversation_id: 1052680788389507073
---
@yudapearl @tdietterich Lots of people who do "interpretable ML" agree that a big problem is that it's hard to know what we want. OTOH I can't define "clear computer program" either, but that does seem an important thing to aim at. Do all software engineers have implicit causal model of their code?