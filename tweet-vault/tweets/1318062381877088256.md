---
id: 1318062381877088256
author: jacobmbuckman
published_date: 2020-10-19 05:32:14 +00:00
conversation_id: 1317532392442286080
in_reply_to: 1318062216453738496
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- states
- preference
- observations
- all
- choose
- easy
- convert
- pomdp
- handle
- s
- randomization
- simply
- turns
- info
- addition

---

@yudapearl @danijarh It turns out this is easy to handle. We can convert POMDP observations to Markovian states by simply concatenating all observations. In addition to the "about to choose a school" partial-state S, we also have preference info M/H, and randomization info choose=C/random=R.  5/

[View tweet on Twitter.com](https://twitter.com/jacobmbuckman/status/1318062381877088256)

### Metadata

Author: [[@jacobmbuckman]]
Conversation: [[conversation-1317532392442286080]]
In reply to: [[1318062216453738496]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2020-10-19]]
keywords:
- [[index/states|states]]
- [[index/preference|preference]]
- [[index/observations|observations]]
- [[index/all|all]]
- [[index/choose|choose]]
- [[index/easy|easy]]
- [[index/convert|convert]]
- [[index/pomdp|pomdp]]
- [[index/handle|handle]]
- [[index/s|s]]
- [[index/randomization|randomization]]
- [[index/simply|simply]]
- [[index/turns|turns]]
- [[index/info|info]]
- [[index/addition|addition]]
