---
id: 1297936223692972032
author: GaryMarcus
published_date: 2020-08-24 16:38:04 +00:00
conversation_id: 1296678240271171585
in_reply_to: 1297924683472318465
retweet: None
quoted_tweet: None
type: tweet
tags:
keywords:
- embedding
- why
- causal
- learning
- start
- cc
- even
- variables
- representation
- conflate
- think
- often
- architecture

---

@tdietterich @neuro_data @KordingLab @tdverstynen @danilobzdok @lakens @TonyZador @ylecun @criticalneuro @IrisVanRooij @neurograce @achristensen56 @AudeOliva @BlackInNeuro @BlackWomenInAI @tyrell_turing and—crucially— is the architecture of GPT even *compatible* with the representation and learning of causal relations? or do you have to start over? n-grams aren’t compatible; why think GPT’s embedding, which often conflate many variables, are?

cc @yudapearl

[View tweet on Twitter.com](https://twitter.com/GaryMarcus/status/1297936223692972032)

### Metadata

Author: [[@GaryMarcus]]
Conversation: [[conversation-1296678240271171585]]
In reply to: [[1297924683472318465]]
Retweet of: [[None]]
Quoted tweet: [[None]]
Published Date: [[calendar/2020-08-24]]
keywords:
- [[index/embedding|embedding]]
- [[index/why|why]]
- [[index/causal|causal]]
- [[index/learning|learning]]
- [[index/start|start]]
- [[index/cc|cc]]
- [[index/even|even]]
- [[index/variables|variables]]
- [[index/representation|representation]]
- [[index/conflate|conflate]]
- [[index/think|think]]
- [[index/often|often]]
- [[index/architecture|architecture]]
