---
id: 1126690824438239235
author: 2309105822
published_date: 2019-05-10 03:30:00 +00:00
conversation_id: [[Conversation-1065280340669816832]]
in_reply_to: [[1126687321212870658]]
retweet: None
quoted_tweet: None
---
@eigenhector @yudapearl @eliasbareinboim @jasonhartford @dustinvtran @zacharylipton @CsabaSzepesvari @GaryMarcus @earnmyturns @ShalitUri Model-based RL makes the counterfactual model p(s_tp1|s, a) explicit. But Q basically encodes p(r_tp1|s, a), which is also counterfactual (in the ways that matter). An agent that maximizes reward in un-memorized envs must implicitly understand p(s_tp1|s, a).
