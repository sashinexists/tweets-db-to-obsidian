---
id: 1297721579325894657
author: GaryMarcus
published_date: 2020-08-24 02:25:09 +00:00
conversation_id: 1296678240271171585
in_reply_to: 1296678240271171585
retweet: None
quoted_tweet: None
type: tweet
tags:

---

@neuro_data @KordingLab @tdverstynen @danilobzdok @lakens @TonyZador @ylecun @criticalneuro @IrisVanRooij @neurograce @achristensen56 @AudeOliva @BlackInNeuro @BlackWomenInAI @tyrell_turing Biology learns about things in the world; ML learns about relations between meaningless tokens. The contrast is stark when you compare GPT with a human child. GPT can (with immense corpus) become more fluent than a child, but it easily gets fooled. Here's a great example [1/2]

### Metadata

Author: [[@GaryMarcus]]
Conversation: [[conversation-1296678240271171585]]
In reply to: [[1296678240271171585]]
Retweet of: [[None]]
Quoted tweet: [[None]]
